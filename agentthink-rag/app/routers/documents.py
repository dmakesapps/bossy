"""
Document Management Router

Handles document upload, processing status, and deletion.
"""

import logging
import shutil
import os
import time
from pathlib import Path

from fastapi import (
    APIRouter,
    UploadFile,
    File,
    Form,
    HTTPException,
    Request,
    BackgroundTasks,
    status
)

from app.models.document import (
    DocumentUploadResponse,
    DocumentStatusResponse,
    DocumentDeleteResponse,
    DocumentListResponse,
    DocumentListItem
)
from app.services.processor import (
    DocumentProcessor,
    get_status,
    update_status,
    clear_status
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents")


def get_processor(request: Request) -> DocumentProcessor:
    """Get processor from app state."""
    return request.app.state.processor


def get_config(request: Request):
    """Get config from app state."""
    return request.app.state.config


@router.post("/upload", response_model=DocumentUploadResponse, status_code=status.HTTP_202_ACCEPTED)
async def upload_document(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    project_id: str = Form(...),
    document_id: str = Form(...)
):
    """
    Upload a document for processing.
    
    The document will be parsed, chunked, embedded, and stored in the vector database.
    Processing happens in the background — use the status endpoint to check progress.
    
    Args:
        file: The document file (PDF, DOCX, CSV, TXT, MD)
        project_id: Project identifier for namespace isolation
        document_id: Unique document identifier (generated by frontend)
    
    Returns:
        202 Accepted with processing status
    """
    config = get_config(request)
    processor = get_processor(request)
    
    # Get filename and extension
    filename = file.filename or "unknown"
    file_ext = Path(filename).suffix.lower().lstrip('.')
    
    # Validate file extension
    if file_ext not in config.ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported file type: .{file_ext}. Allowed: {config.ALLOWED_EXTENSIONS}"
        )
    
    # Check file size
    file.file.seek(0, 2)  # Seek to end
    file_size = file.file.tell()
    file.file.seek(0)  # Reset to beginning
    
    if file_size > config.max_file_size_bytes:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=f"File too large. Maximum size: {config.MAX_FILE_SIZE_MB}MB"
        )
    
    # Create upload directory
    upload_dir = Path(config.UPLOAD_DIR) / project_id / document_id
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    # Save file to disk
    file_path = upload_dir / filename
    
    try:
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        logger.info(f"Saved file: {file_path} ({file_size} bytes)")
        
    except Exception as e:
        logger.exception(f"Failed to save file: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to save uploaded file"
        )
    
    # Set initial processing status
    update_status(document_id, "processing")
    
    # Start background processing
    background_tasks.add_task(
        _process_document_task,
        processor,
        str(file_path),
        project_id,
        document_id,
        filename
    )
    
    logger.info(f"Document queued for processing: {document_id} ({filename})")
    
    return DocumentUploadResponse(
        status="processing",
        document_id=document_id,
        message="Document queued for processing"
    )


async def _process_document_task(
    processor: DocumentProcessor,
    file_path: str,
    project_id: str,
    document_id: str,
    filename: str
):
    """Background task to process a document."""
    try:
        result = processor.process_document(
            file_path=file_path,
            project_id=project_id,
            document_id=document_id,
            filename=filename
        )
        
        # Status is updated by the processor
        logger.info(
            f"Document processing completed: {document_id} "
            f"(status: {result.status}, chunks: {result.chunk_count})"
        )
        
    except Exception as e:
        logger.exception(f"Background processing failed for {document_id}: {e}")
        update_status(document_id, "error", error_message=str(e))


@router.get("/list", response_model=DocumentListResponse)
async def list_documents(request: Request, project_id: str):
    """
    List all documents for a given project.
    
    Fetches documents from the upload directory and merges with in-memory status.
    If a document exists on disk but not in memory (e.g. after restart),
    we query Qdrant for the real chunk count to determine the true status.
    
    Args:
        project_id: Project identifier
    
    Returns:
        List of documents with status
    """
    config = get_config(request)
    vector_store = request.app.state.vector_store
    project_dir = Path(config.UPLOAD_DIR) / project_id
    
    documents = []
    
    if project_dir.exists():
        # Iterate over document directories
        for doc_dir in project_dir.iterdir():
            if not doc_dir.is_dir():
                continue
                
            document_id = doc_dir.name
            
            # Find the file inside
            files = list(doc_dir.glob("*"))
            if not files:
                continue
                
            file_path = files[0]
            filename = file_path.name
            
            # Get creation time
            created_at = file_path.stat().st_ctime
            
            # Get status from memory first
            status_info = get_status(document_id)
            
            if status_info:
                doc_status = status_info.status
                chunk_count = status_info.chunk_count
                error_message = status_info.error_message
            else:
                # No in-memory status (e.g. after container restart).
                # Query Qdrant for the real chunk count.
                chunk_count = vector_store.count_document_chunks(project_id, document_id)
                
                if chunk_count > 0:
                    doc_status = "ready"
                    error_message = None
                    # Restore the in-memory status so subsequent calls are fast
                    update_status(document_id, "ready", chunk_count=chunk_count)
                else:
                    # File exists on disk but has no indexed chunks — needs reprocessing
                    doc_status = "needs_reprocessing"
                    error_message = "Document exists but has no indexed chunks. Please re-upload."
            
            # Validation: never report 'ready' with chunk_count=0
            if doc_status == "ready" and chunk_count == 0:
                doc_status = "needs_reprocessing"
                error_message = "Document marked ready but has no indexed chunks."
            
            documents.append(DocumentListItem(
                document_id=document_id,
                filename=filename,
                status=doc_status,
                chunk_count=chunk_count,
                created_at=created_at,
                error_message=error_message
            ))
    
    # Sort by creation time descending (newest first)
    documents.sort(key=lambda x: x.created_at, reverse=True)
    
    return DocumentListResponse(documents=documents)


@router.get("/{document_id}/status", response_model=DocumentStatusResponse)
async def get_document_status(document_id: str):
    """
    Get the processing status of a document.
    
    Args:
        document_id: The document identifier
    
    Returns:
        Document processing status including chunk count if ready
    """
    status_response = get_status(document_id)
    
    if status_response is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Document not found"
        )
    
    return status_response


@router.delete("/{document_id}", response_model=DocumentDeleteResponse)
async def delete_document(
    request: Request,
    document_id: str,
    project_id: str
):
    """
    Delete a document and its chunks from the vector store.
    
    Args:
        document_id: The document identifier
        project_id: Project identifier (query parameter)
    
    Returns:
        Deletion status with count of removed chunks
    """
    config = get_config(request)
    processor = get_processor(request)
    
    # Delete chunks from Qdrant
    chunks_removed = processor.delete_document(project_id, document_id)
    
    # Delete files from disk
    upload_dir = Path(config.UPLOAD_DIR) / project_id / document_id
    
    if upload_dir.exists():
        try:
            shutil.rmtree(upload_dir)
            logger.info(f"Deleted upload directory: {upload_dir}")
        except Exception as e:
            logger.warning(f"Failed to delete upload directory: {e}")
    
    # Remove from status tracking
    clear_status(document_id)
    
    logger.info(f"Document deleted: {document_id} ({chunks_removed} chunks)")
    
    return DocumentDeleteResponse(
        status="deleted",
        chunks_removed=chunks_removed
    )


@router.post("/{document_id}/reprocess", response_model=DocumentUploadResponse, status_code=status.HTTP_202_ACCEPTED)
async def reprocess_document(
    request: Request,
    background_tasks: BackgroundTasks,
    document_id: str,
    project_id: str
):
    """
    Re-process a document that exists on disk but has no indexed chunks.
    
    This is useful after a container restart if the original processing
    failed silently, or if chunks were lost from the vector store.
    
    Args:
        document_id: The document identifier
        project_id: Project identifier (query parameter)
    
    Returns:
        202 Accepted with processing status
    """
    config = get_config(request)
    processor = get_processor(request)
    
    # Find the file on disk
    doc_dir = Path(config.UPLOAD_DIR) / project_id / document_id
    
    if not doc_dir.exists():
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document directory not found: {document_id}"
        )
    
    files = list(doc_dir.glob("*"))
    if not files:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No file found in document directory: {document_id}"
        )
    
    file_path = files[0]
    filename = file_path.name
    
    # Set initial processing status
    update_status(document_id, "processing")
    
    # Start background processing
    background_tasks.add_task(
        _process_document_task,
        processor,
        str(file_path),
        project_id,
        document_id,
        filename
    )
    
    logger.info(f"Document queued for reprocessing: {document_id} ({filename})")
    
    return DocumentUploadResponse(
        status="processing",
        document_id=document_id,
        message="Document queued for reprocessing"
    )
